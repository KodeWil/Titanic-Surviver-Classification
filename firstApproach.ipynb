{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import preprocessing\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom IPython.display import Image, display\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn import tree\nfrom sklearn import utils\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import tree\n\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport graphviz\nimport matplotlib.pyplot as plt\nimport sys\nimport string\nimport csv\nimport random\nimport copy\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def randomForest(clfs,estimators,featuresFracs,crit):\n    for criter in crit:\n        for estimator in estimators:\n            for featuresFrac in featuresFracs:   \n                  clfs.append(RandomForestClassifier(\n                      n_estimators = estimator,\n                      #integer, optional (default=100)\n                      criterion= criter,\n                      #string, optional (default=”gini”)\n                      max_depth= None,\n                      #integer or None, optional (default=None)\n                      min_samples_split=2,\n                      #int, float, optional (default=2)\n                      min_samples_leaf=1,\n                      #int, float, optional (default=1)\n                      min_weight_fraction_leaf=0.0,\n                      #float, optional (default=0.)\n                      max_features= featuresFrac,\n                      #int, float, string or None, optional (default=”auto”)\n                      max_leaf_nodes=None,\n                      #int or None, optional (default=None)\n                      min_impurity_decrease=0.01,\n                      #float, optional (default=0.)\n                      bootstrap=True,\n                      #boolean, optional (default=True)\n                      oob_score=False,\n                      #bool (default=False)\n                      n_jobs=None,\n                      #int or None, optional (default=None)\n                      random_state=None,\n                      #int, RandomState instance or None, optional (default=None)\n                      verbose=0,\n                      #int, optional (default=0)\n                      warm_start=False,\n                      #bool, optional (default=False)\n                      ccp_alpha=0.0,\n                      #non-negative float, optional (default=0.0)\n                      max_samples=None\n                      #int or float, default=None\n                      ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decisionTree(clfs, crits, impDecreases):    \n    for crit in crits:\n        for imp in impDecreases:\n            clf = tree.DecisionTreeClassifier(\n                  criterion= \"entropy\",\n                  splitter= crit,\n                  max_depth=None,\n                  min_samples_split= 2,\n                  min_samples_leaf=5,\n                  min_weight_fraction_leaf=0.0,\n                  max_features=None,\n                  random_state=None,\n                  max_leaf_nodes=None,\n                  min_impurity_decrease=imp,\n                  class_weight=None,\n                  ccp_alpha = 0.0)\n            clfs.append(clf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def moreImportant(importance):\n  arrSorted = copy.deepcopy(importance)\n  arrSorted.sort()\n  arrSorted = arrSorted[::-1]\n  indxs = []\n  for i in range(0,5):\n    temp = np.where(arrSorted[i] == importance)\n    temp = int(temp[0])\n    indxs.append(temp)\n  \n  return indxs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def removeAditiveName(names):\n#     names = {x.replace('Miss', ' ').replace('Mr', ' ').replace('Mrs', ' ').replace('Dr', ' ').replace('Capt', ' ').replace('Master', ' ').replace('Mme', ' ').replace('Mlle', ' ').replace('Sir', ' ').replace('Col', ' ').replace('Rev', ' ') for x in names}\n#     names = {x.replace('  ', ' ') for x in names}\n    newNames = []\n    for name in names:\n        name = removePunctuation(name)\n        newNames.append(name)\n    return newNames\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def removePunctuation(name):\n    newName = ''\n    for ch in name:\n        if ch not in string.punctuation:\n            newName = newName + ch \n    \n    return newName","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def removeNoNumber(name):\n    newName = ''\n    digits = '0123456789'\n    for ch in name:\n        if ch in digits:\n            newName = newName + ch\n    if(newName == ''):\n        return 0\n    return float(newName)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def printAll(value):\n    with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n        display(value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotImportance(forest, X, y):\n    forest.fit(X, y)\n    importances = forest.feature_importances_\n    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n                 axis=0)\n    indices = np.argsort(importances)[::-1]\n\n    # Print the feature ranking\n    print(\"Feature ranking:\")\n\n    for f in range(X.shape[1]):\n        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n    # Plot the feature importances of the forest\n    plt.figure()\n    plt.title(\"Feature importances\")\n    plt.bar(range(X.shape[1]), importances[indices],\n           color=\"r\", yerr=std[indices], align=\"center\")\n    plt.xticks(range(X.shape[1]), indices)\n    plt.xlim([-1, X.shape[1]])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read the csv and extract the data in labels,X and y\nX = pd.read_csv('/kaggle/input/titanic/train.csv')\ny = X['Survived']\nX = X.drop(columns=\"Survived\")\n\n#Preprocessing to convert string data in numerical data\n\n#Select all no numerical Data\nprocX = X.select_dtypes(include=[object])\n\n#Fill NaN for numerical data with 0\nfor column in X.columns:\n    columnName = str(column)\n    X[columnName].fillna(0, inplace = True)\n#Fill NaN for numerical data with 0, in this case only Cabin and Embarked has NaN     \nprocX['Cabin'].fillna('0', inplace = True)\nprocX['Embarked'].fillna('0', inplace = True)\n\n#Create the preprocessing Label encoder, apply it to the no numerical data and replace the result of the last step in ...\n# the original data frame (X)\nle = preprocessing.LabelEncoder()\nprocX = procX.apply(le.fit_transform)\nX['Name'] = procX['Name']\nX['Sex'] = procX['Sex']\nX['Ticket'] = procX['Ticket']\nX['Cabin'] = procX['Cabin']\nX['Embarked'] = procX['Embarked']\n\n#Train a decision tree to 'estimate' the age of passengers without this feature based on the other ones\n\n#Extracting the data for train and test the decision tree \ny_age = X['Age'][X['Age'] != 0]\ny_age = y_age.tolist()      \nX_age = X[X['Age'] != 0]\nX_age = X_age.drop(columns = 'Age')\nX_age.insert(1, 'Survived', y[X['Age'] != 0])\n\n#Split train and test data\nX_train, X_test, y_train, y_test = train_test_split(X_age, y_age, test_size=0.30, random_state=None)\n\n#Set the test values for variables in tree (criterio and minImpurity Decrease)\nclfs = []\ncrits = ['random']\nimpDecreases = [0.1]\ndecisionTree(clfs, crits, impDecreases)\nbest = []\n\n#Split train and test data\nX_train, X_test, y_train, y_test = train_test_split(X_age, y_age, test_size=0.30, random_state=None)\n\n#Change all set's features to multiclass instead of continuos\ny_train = le.fit_transform(y_train)\ny_test = le.fit_transform(y_test)\na,b =  X_train.shape\nfor row in range(0, a):\n    X_train.iloc[row] = le.fit_transform(X_train.iloc[row])\na,b =  X_test.shape    \nfor row in range(0, a):\n    X_test.iloc[row] = le.fit_transform(X_test.iloc[row])\n\n#Train\ntestTrees = []\nfor clf in clfs:\n    tempClf = clf.fit(X_train, y_train)\n    testTrees.append(tempClf)\n    y_pred = tempClf.predict(X_test)\n\n#Fill the rows with NaN in age column \nemptyAge =  X[X['Age'] == 0]\nemptyAge = emptyAge.drop(columns = 'Age')\nemptyAge.insert(1, 'Survived', y[X['Age'] == 0])\npredAge = clf.predict(emptyAge)\nX['Age'][X['Age'] == 0] = predAge\n\n#Split train and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=None)\n\n#Train and test the tree classifier \nscores = []\nforests = []\nestimators = [1000]\ncrit = ['gini']\nfeaturesFracs = [0.99]\nrandomForest(forests,estimators,featuresFracs,crit)\nimportance = []\nprint(len(trees))\nfor clf in trees:\n    clf.fit(X_train, y_train)\n    scores.append(clf.score( X_test, y_test))\n    importance.append(clf.feature_importances_)\nprint(scores)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}