{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import preprocessing\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom IPython.display import Image, display\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn import tree\nfrom sklearn import utils\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import tree\nfrom sklearn import preprocessing\nfrom sklearn import svm\n\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport graphviz\nimport matplotlib.pyplot as plt\nimport sys\nimport string\nimport csv\nimport random\nimport copy\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/gender_submission.csv\n/kaggle/input/titanic/test.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gradientRegressor(clfs, lRate, estimators, depths):\n    for rate in lRate:\n        for trees in estimators:\n            for depth in depths:\n                clf = GradientBoostingRegressor(loss='ls', \n                                          learning_rate = 0.1, \n                                          n_estimators = 100, \n                                          subsample = 1.0, \n                                          criterion = 'friedman_mse', \n                                          min_samples_split = 2, \n                                          min_samples_leaf = 1, \n                                          min_weight_fraction_leaf = 0.0, \n                                          max_depth = 3, \n                                          min_impurity_decrease = 0.0, \n                                          min_impurity_split = None, \n                                          init = None, \n                                          random_state = None, \n                                          max_features = None, \n                                          alpha = 0.9, \n                                          verbose = 0, \n                                          max_leaf_nodes = None, \n                                          warm_start = False, \n                                          presort = 'deprecated', \n                                          validation_fraction = 0.1, \n                                          n_iter_no_change = None, \n                                          tol = 0.0001, \n                                          ccp_alpha = 0.0)\n                clfs.append(clf)","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def randomForest(clfs,estimators,featuresFracs,crit):\n    for criter in crit:\n        for estimator in estimators:\n            for featuresFrac in featuresFracs:   \n                  clfs.append(RandomForestClassifier(\n                      n_estimators = estimator,\n                      #integer, optional (default=100)\n                      criterion= criter,\n                      #string, optional (default=”gini”)\n                      max_depth= None,\n                      #integer or None, optional (default=None)\n                      min_samples_split=2,\n                      #int, float, optional (default=2)\n                      min_samples_leaf=1,\n                      #int, float, optional (default=1)\n                      min_weight_fraction_leaf=0.0,\n                      #float, optional (default=0.)\n                      max_features= featuresFrac,\n                      #int, float, string or None, optional (default=”auto”)\n                      max_leaf_nodes=None,\n                      #int or None, optional (default=None)\n                      min_impurity_decrease=0.01,\n                      #float, optional (default=0.)\n                      bootstrap=True,\n                      #boolean, optional (default=True)\n                      oob_score=False,\n                      #bool (default=False)\n                      n_jobs=None,\n                      #int or None, optional (default=None)\n                      random_state=None,\n                      #int, RandomState instance or None, optional (default=None)\n                      verbose=0,\n                      #int, optional (default=0)\n                      warm_start=False,\n                      #bool, optional (default=False)\n                      ccp_alpha=0.0,\n                      #non-negative float, optional (default=0.0)\n                      max_samples=None\n                      #int or float, default=None\n                      ))","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decisionTree(clfs, crits, impDecreases):    \n    for crit in crits:\n        for imp in impDecreases:\n            clf = tree.DecisionTreeClassifier(\n                  criterion= \"entropy\",\n                  splitter= crit,\n                  max_depth=None,\n                  min_samples_split= 2,\n                  min_samples_leaf=5,\n                  min_weight_fraction_leaf=0.0,\n                  max_features=None,\n                  random_state=None,\n                  max_leaf_nodes=None,\n                  min_impurity_decrease=imp,\n                  class_weight=None,\n                  ccp_alpha = 0.0)\n            clfs.append(clf)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gradientBoost(clfs, estimators):\n    for estimator in estimators:\n        clf = GradientBoostingClassifier(loss='deviance', \n                                   learning_rate = 0.1, \n                                   n_estimators = estimator, \n                                   subsample = 1.0, \n                                   criterion = 'friedman_mse', \n                                   min_samples_split = 2 , \n                                   min_samples_leaf = 1, \n                                   min_weight_fraction_leaf = 0.0, \n                                   max_depth = 3, \n                                   min_impurity_decrease = 0.0, \n                                   min_impurity_split = None, \n                                   init = None, \n                                   random_state = None, \n                                   max_features = None, \n                                   verbose = 0, \n                                   max_leaf_nodes = None, \n                                   warm_start = False, \n                                   presort = 'deprecated', \n                                   validation_fraction = 0.1, \n                                   n_iter_no_change = None, \n                                   tol = 0.0001, \n                                   ccp_alpha = 0.0)\n        clfs.append(clf)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bestNeighbors(clfs, neighbors, leafSizes):\n    for neighbor in neighbors:\n        for leafSize in leafSizes:\n            clf = KNeighborsClassifier(n_neighbors = 5, \n                                 weights = 'uniform', \n                                 algorithm = 'auto', \n                                 leaf_size = 30, \n                                 p = 2, \n                                 metric = 'minkowski', \n                                 metric_params = None,\n                                 n_jobs = None,)\n            clfs.append(clf)\n    ","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def printAll(value):\n    with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n        display(value)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotImportance(forest, X, y):\n    forest.fit(X, y)\n    importances = forest.feature_importances_\n    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n                 axis=0)\n    indices = np.argsort(importances)[::-1]\n\n    # Plot the feature importances of the forest\n    plt.figure()\n    plt.title(\"Feature importances\")\n    plt.bar(range(X.shape[1]), importances[indices],\n           color=\"r\", yerr=std[indices], align=\"center\")\n    plt.xticks(range(X.shape[1]), X.columns)\n    plt.xlim([-1, X.shape[1]])\n    plt.show()\n","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fillAge(X_age, y_age, X, y, train):\n    #Split train and test data\n    X_train, X_test, y_train, y_test = train_test_split(X_age, y_age, test_size=0.30, random_state=None)\n\n    #Set the test values for variables in tree (criterio and minImpurity Decrease)\n    clfs = []\n    lRate = [0.05, 0.1, 0.15, 0.2]\n    estimators = [100, 200, 500]\n    depths = [3, 4, 5]\n    gradientRegressor(clfs, lRate, estimators, depths)\n\n    #Train\n    testTrees = []\n    minErr = 9999\n    regressor = None\n    for clf in clfs:\n        tempClf = clf.fit(X_train, y_train)\n        testTrees.append(tempClf)\n        tempScore = abs(tempClf.score(X_test, y_test))\n        if(tempScore < minErr):\n            minErr = tempScore\n            regressor = tempClf\n        \n    #Fill the rows with NaN in age column \n    emptyAge =  X[X['Age'] == 0]\n    emptyAge = emptyAge.drop(columns = 'Age')\n    if train == 1:\n        emptyAge.insert(1, 'Survived', y[X['Age'] == 0])\n    predAge = regressor.predict(emptyAge)\n    predAge = [int(age) for age in predAge]\n    X['Age'][X['Age'] == 0] = predAge","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def trainClassifier(clfs, X, y, X_F, ids):\n    \n    #Split train and test data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=None)\n\n\n    scores = []\n    importance = []\n    #Find best classifier\n    for clf in clfs:\n        clf.fit(X_train, y_train)\n        scores.append(clf.score(X_test, y_test))\n    #Add XGBoost and SVM\n    \n    #XGBoost\n    model = XGBClassifier()\n    model.fit(X_train, y_train)\n    scores.append(model.score(X_test, y_test))\n    clfs.append(model)\n    \n    #SVM\n    model = svm.SVC()\n    model.fit(X_train, y_train)\n    scores.append(model.score(X_test, y_test))\n    clfs.append(model)\n    \n    #Find the best classifier for the test\n    bestIndx = scores.index(max(scores))\n    print('best: ' + str(bestIndx) + ' score: ' + str(max(scores)))\n    best = clfs[bestIndx]\n    best = best.fit(X, y)\n    \n    #The poorest predictions will be replaced for a prediction based on k-nearest neighbors\n    #Save indices' of the poorest predictions (Almost random) if method exists\n    Survived = best.predict(X_F)#Predict survivers with gradient boost classifier\n    if bestIndx < 9:\n        predProb = best.predict_proba(X_F) #Predict probabilities of being in 0/1 clas\n        count = 0\n        indices = [] \n        for row in predProb:\n            dif = abs(row[0] - row[1])\n            if dif < 0.15:\n                indices.append(count)\n            count = count + 1\n        poorPred = X_F.loc[indices, :]\n        print('PoorNumber: ' + str(len(indices)))\n        neighbors = [] #List with different KNeighborsClassifier to select the better one \n        numNeighbors = [3, 5, 10]\n        lSizes = [25, 30, 35]\n        bestNeighbors(neighbors, numNeighbors, lSizes)\n        best = 0\n        scores = []\n        for neighbor in neighbors:\n            temp = neighbor.fit(X_train, y_train)\n            scores.append(temp.score( X_test, y_test))\n\n        #Find the best classifier for the test \n        best = scores.index(max(scores))\n        best = neighbors[best]\n        best = best.fit(X, y)\n        newPred = best.predict(poorPred) #New prediction for the poorest predictions\n        Survived[indices] = newPred\n\n    #Create dataframe to submit \n    df = pd.DataFrame(columns = ['PassengerId', 'Survived']) \n    df['Survived'] = Survived\n    df['PassengerId'] = ids\n    df.to_csv(r'/kaggle/working/train.csv')\n    \n    return max(scores)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read the csv and extract the data in labels,X and y\nX = pd.read_csv('/kaggle/input/titanic/train.csv')\ny = X['Survived']\nX.drop(columns=\"Survived\", inplace = True)\nX_testF = pd.read_csv('/kaggle/input/titanic/test.csv')\n\n#Preprocessing to convert string data in numerical data\n\n#Select all no numerical Data\nprocX = X.select_dtypes(include=[object])\nprocXt = X_testF.select_dtypes(include=[object])\n\n#Fill NaN for numerical data with 0\n\n#Train data\nfor column in X.columns:\n    columnName = str(column)\n    X[columnName].fillna(0, inplace = True)\n    \n#Test data\nfor column in X_testF.columns:\n    columnName = str(column)\n    X_testF[columnName].fillna(0, inplace = True)\n\n#Drop poorest related features\n\n#Train data\nX.drop(columns = 'Name', inplace = True)\nX.drop(columns = 'Ticket', inplace = True)\nX.drop(columns = 'Cabin', inplace = True)\nX.drop(columns = 'Fare', inplace = True)\nX.drop(columns = 'PassengerId', inplace = True)\n\n#Test data\nX_testF.drop(columns = 'Name', inplace = True)\nX_testF.drop(columns = 'Ticket', inplace = True)\nX_testF.drop(columns = 'Cabin', inplace = True)\nX_testF.drop(columns = 'Fare', inplace = True)    \npId = X_testF.drop(columns = 'PassengerId', inplace = True)\n\n\n#Train data\n#map Sex, age and embarked port value to a numerical value\nX['Age'] = pd.cut(x = X['Age'], bins = [0, 10, 20, 30, 40, 50, 60, 80], labels = ['Infants','Teenagers','20s', '30s','40s','50s','Oldies'])\nage_mapping = {'Infants': 0, 'Teenagers': 1, '20s': 2, '30s': 3, '40s': 4, '50s': 5, 'Oldies': 6}\nX['Age'] = X['Age'].map(age_mapping)\nX['Age'] = pd.to_numeric(X['Age'], downcast='signed')\nsex_mapping = {\"male\": 0, \"female\": 1}\nX['Sex'] = X['Sex'].map(sex_mapping)\nport_mapping = {\"S\": 0, \"C\": 1, \"Q\": 2}\nX['Embarked'] = X['Embarked'].map(port_mapping)\n\n#Test data\nX_testF['Age'] = pd.cut(x = X_testF['Age'], bins = [0, 10, 20, 30, 40, 50, 60, 80], labels = ['Infants','Teenagers','20s', '30s','40s','50s','Oldies'])\nX_testF['Age'] = X_testF['Age'].map(age_mapping)\nX_testF['Age'] = pd.to_numeric(X_testF['Age'], downcast='signed')\nX_testF['Sex'] = X_testF['Sex'].map(sex_mapping)\nX_testF['Embarked'] = X_testF['Embarked'].map(port_mapping)\n\n#Fill NaN for numerical data with 0, in this case only Embarked and Age has NaN     \n\n#Train data\nX['Embarked'].fillna(0, inplace = True)\nX['Age'].fillna(0, inplace = True)\n\n#Test data \nX_testF['Embarked'].fillna(0, inplace = True)\nX_testF['Age'].fillna(0, inplace = True)\n\n#Predict age \n\n#train data\ny_age = X['Age'][X['Age'] != 0]\ny_age = y_age.tolist()      \nX_age = X[X['Age'] != 0]\nX_age = X_age.drop(columns = 'Age')\nX_age.insert(1, 'Survived', y[X['Age'] != 0])\n\n\n#test data\ny_aget = X_testF['Age'][X_testF['Age'] != 0]\ny_aget = y_aget.tolist()      \nX_aget = X_testF[X_testF['Age'] != 0]\nX_aget = X_aget.drop(columns = 'Age')\n\n# Fill NaN age with predicted age \nfillAge(X_age, y_age, X, y, 1)\nfillAge(X_aget, y_aget, X_testF, 0, 0)\n\nfinal = 0\n#Train and test the classifiers \niterCount = 0\n\n#Iterate until it find an appropiate classifier +84%\nwhile(final < 0.84):\n    #Random forest\n    forests = []\n    estimators = [100, 500, 1000]\n    crit = ['gini', 'entropy']\n    featuresFracs = [0.99]\n    randomForest(forests,estimators,featuresFracs,crit)\n\n    #Gradient Boosting Classifier\n    estimators = [100, 500, 1000]\n    gradientBoost(forests, estimators)\n    final = trainClassifier(forests, X, y, X_testF, pId)\n    iterCount = iterCount + 1\n    print('IterCount: ' + str(iterCount))\n    print('')\nprint(':)')","execution_count":12,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","name":"stderr"},{"output_type":"stream","text":"best: 6 score: 0.8022388059701493\nPoorNumber: 45\nIterCount: 1\n\nbest: 0 score: 0.8134328358208955\nPoorNumber: 42\nIterCount: 2\n\nbest: 0 score: 0.8097014925373134\nPoorNumber: 42\nIterCount: 3\n\nbest: 10 score: 0.8470149253731343\nIterCount: 4\n\n:)\n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}